{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0a1778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>plans</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f15989250b8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import * #for window\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".master(\"yarn\")\\\n",
    ".appName(\"plans\")\\\n",
    ".enableHiveSupport()\\\n",
    ".config(\"spark.shuffle.useOldFetchProtocol\",'true')\\\n",
    ".config(\"spark.sql.warehouse.dir\",\"/user/itv009490/warehouse\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fcd0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id  long, order_date string, customer_id long, order_status string '\n",
    "orders_df = spark.read.format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/public/trendytech/orders/orders_1gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50361405",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26aa9538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fed4b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "|       3|       COMPLETE|\n",
      "|       4|         CLOSED|\n",
      "|       5|       COMPLETE|\n",
      "|       6|       COMPLETE|\n",
      "|       7|       COMPLETE|\n",
      "|       8|     PROCESSING|\n",
      "|       9|PENDING_PAYMENT|\n",
      "|      10|PENDING_PAYMENT|\n",
      "|      11| PAYMENT_REVIEW|\n",
      "|      12|         CLOSED|\n",
      "|      13|PENDING_PAYMENT|\n",
      "|      14|     PROCESSING|\n",
      "|      15|       COMPLETE|\n",
      "|      16|PENDING_PAYMENT|\n",
      "|      17|       COMPLETE|\n",
      "|      18|         CLOSED|\n",
      "|      19|PENDING_PAYMENT|\n",
      "|      20|     PROCESSING|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select order_id,order_status from (select order_id,customer_id,order_status from orders where order_id<500)\n",
    "where order_id<200\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df20ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['order_id, 'order_status]\n",
      "+- 'Filter ('order_id < 200)\n",
      "   +- 'SubqueryAlias __auto_generated_subquery_name\n",
      "      +- 'Project ['order_id, 'customer_id, 'order_status]\n",
      "         +- 'Filter ('order_id < 500)\n",
      "            +- 'UnresolvedRelation [orders], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_id: bigint, order_status: string\n",
      "Project [order_id#0L, order_status#3]\n",
      "+- Filter (order_id#0L < cast(200 as bigint))\n",
      "   +- SubqueryAlias __auto_generated_subquery_name\n",
      "      +- Project [order_id#0L, customer_id#2L, order_status#3]\n",
      "         +- Filter (order_id#0L < cast(500 as bigint))\n",
      "            +- SubqueryAlias orders\n",
      "               +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [order_id#0L, order_status#3]\n",
      "+- Filter ((isnotnull(order_id#0L) AND (order_id#0L < 500)) AND (order_id#0L < 200))\n",
      "   +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter ((isnotnull(order_id#0L) AND (order_id#0L < 500)) AND (order_id#0L < 200))\n",
      "+- FileScan csv [order_id#0L,order_status#3] Batched: false, DataFilters: [isnotnull(order_id#0L), (order_id#0L < 500), (order_id#0L < 200)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_id), LessThan(order_id,500), LessThan(order_id,200)], ReadSchema: struct<order_id:bigint,order_status:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select order_id,order_status from (select order_id,customer_id,order_status from orders where order_id<500)\n",
    "where order_id<200\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef3e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c51c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcb669de",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_schema = \"\"\"customerid long, customer_fname string, customer_lname string, username string, password string,\n",
    "address string, city string, state string, pincode long\"\"\"\n",
    "customers_df = spark.read.format(\"csv\").schema(customers_schema).load(\"/public/trendytech/retail_db/customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8823091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53a991d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('order_status = CLOSED)\n",
      "   +- 'Join Inner, ('orders.customer_id = 'customers.customerid)\n",
      "      :- 'UnresolvedRelation [orders], [], false\n",
      "      +- 'UnresolvedRelation [customers], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_id: bigint, order_date: string, customer_id: bigint, order_status: string, customerid: bigint, customer_fname: string, customer_lname: string, username: string, password: string, address: string, city: string, state: string, pincode: bigint\n",
      "Project [order_id#0L, order_date#1, customer_id#2L, order_status#3, customerid#48L, customer_fname#49, customer_lname#50, username#51, password#52, address#53, city#54, state#55, pincode#56L]\n",
      "+- Filter (order_status#3 = CLOSED)\n",
      "   +- Join Inner, (customer_id#2L = customerid#48L)\n",
      "      :- SubqueryAlias orders\n",
      "      :  +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "      +- SubqueryAlias customers\n",
      "         +- Relation[customerid#48L,customer_fname#49,customer_lname#50,username#51,password#52,address#53,city#54,state#55,pincode#56L] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (customer_id#2L = customerid#48L)\n",
      ":- Filter ((isnotnull(order_status#3) AND (order_status#3 = CLOSED)) AND isnotnull(customer_id#2L))\n",
      ":  +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "+- Filter isnotnull(customerid#48L)\n",
      "   +- Relation[customerid#48L,customer_fname#49,customer_lname#50,username#51,password#52,address#53,city#54,state#55,pincode#56L] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [customer_id#2L], [customerid#48L], Inner, BuildRight, false\n",
      ":- *(2) Filter ((isnotnull(order_status#3) AND (order_status#3 = CLOSED)) AND isnotnull(customer_id#2L))\n",
      ":  +- FileScan csv [order_id#0L,order_date#1,customer_id#2L,order_status#3] Batched: false, DataFilters: [isnotnull(order_status#3), (order_status#3 = CLOSED), isnotnull(customer_id#2L)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_status), EqualTo(order_status,CLOSED), IsNotNull(customer_id)], ReadSchema: struct<order_id:bigint,order_date:string,customer_id:bigint,order_status:string>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#145]\n",
      "   +- *(1) Filter isnotnull(customerid#48L)\n",
      "      +- FileScan csv [customerid#48L,customer_fname#49,customer_lname#50,username#51,password#52,address#53,city#54,state#55,pincode#56L] Batched: false, DataFilters: [isnotnull(customerid#48L)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/retail_db/customers], PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:bigint,customer_fname:string,customer_lname:string,username:string,password:str...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from orders \n",
    "inner join customers\n",
    "on orders.customer_id == customers.customerid\n",
    "where order_status='CLOSED'\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf76f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['customer_id], ['customer_id, unresolvedalias('count(1), None)]\n",
      "+- 'Filter 'customer_id IN (1,2,3)\n",
      "   +- 'SubqueryAlias __auto_generated_subquery_name\n",
      "      +- 'Project [*]\n",
      "         +- 'Filter 'customer_id IN (1,2,3,4,5)\n",
      "            +- 'UnresolvedRelation [orders], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: bigint, count(1): bigint\n",
      "Aggregate [customer_id#2L], [customer_id#2L, count(1) AS count(1)#172L]\n",
      "+- Filter cast(customer_id#2L as bigint) IN (cast(1 as bigint),cast(2 as bigint),cast(3 as bigint))\n",
      "   +- SubqueryAlias __auto_generated_subquery_name\n",
      "      +- Project [order_id#0L, order_date#1, customer_id#2L, order_status#3]\n",
      "         +- Filter cast(customer_id#2L as bigint) IN (cast(1 as bigint),cast(2 as bigint),cast(3 as bigint),cast(4 as bigint),cast(5 as bigint))\n",
      "            +- SubqueryAlias orders\n",
      "               +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [customer_id#2L], [customer_id#2L, count(1) AS count(1)#172L]\n",
      "+- Project [customer_id#2L]\n",
      "   +- Filter (customer_id#2L IN (1,2,3,4,5) AND customer_id#2L IN (1,2,3))\n",
      "      +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[customer_id#2L], functions=[count(1)], output=[customer_id#2L, count(1)#172L])\n",
      "+- Exchange hashpartitioning(customer_id#2L, 200), ENSURE_REQUIREMENTS, [id=#171]\n",
      "   +- *(1) HashAggregate(keys=[customer_id#2L], functions=[partial_count(1)], output=[customer_id#2L, count#176L])\n",
      "      +- *(1) Filter (customer_id#2L IN (1,2,3,4,5) AND customer_id#2L IN (1,2,3))\n",
      "         +- FileScan csv [customer_id#2L] Batched: false, DataFilters: [customer_id#2L IN (1,2,3,4,5), customer_id#2L IN (1,2,3)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [In(customer_id, [1,2,3,4,5]), In(customer_id, [1,2,3])], ReadSchema: struct<customer_id:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select customer_id,count(1) from (select * from orders where customer_id in (1,2,3,4,5))\n",
    "where customer_id in (1,2,3)\n",
    "group by customer_id\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394df2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
