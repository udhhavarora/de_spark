{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becf2756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>window functions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7eff047d5400>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import * #for window\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".master(\"yarn\")\\\n",
    ".appName(\"window functions\")\\\n",
    ".enableHiveSupport()\\\n",
    ".config(\"spark.sql.warehouse.dir\",\"/user/itv009490/warehouse\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528f329d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a386c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047a76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list = [(\"INFO\",\"2015-8-8 20:49:22\"),\n",
    "(\"WARN\",\"2015-1-14 20:05:00\"),\n",
    "(\"INFO\",\"2017-6-14 00:08:35\"),\n",
    "(\"INFO\",\"2016-1-18 11:50:14\"),\n",
    "(\"DEBUG\",\"2017-7-1 12:55:02\"),\n",
    "(\"INFO\",\"2014-2-26 12:34:21\"),\n",
    "(\"INFO\",\"2015-7-12 11:13:47\"),\n",
    "(\"INFO\",\"2017-4-15 01:20:18\"),\n",
    "(\"DEBUG\",\"2016-11-2 20:19:23\"),\n",
    "(\"INFO\",\"2012-8-20 10:09:44\"),\n",
    "(\"DEBUG\",\"2014-4-22 21:30:49\"),\n",
    "(\"WARN\",\"2013-12-6 17:54:15\"),\n",
    "(\"DEBUG\",\"2017-1-12 10:47:02\"),\n",
    "(\"DEBUG\",\"2016-6-25 11:06:42\"),\n",
    "(\"ERROR\",\"2015-6-28 19:25:05\"),\n",
    "(\"DEBUG\",\"2012-6-24 01:06:37\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00811e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df=spark.createDataFrame(log_list).toDF(\"loglevel\",\"logtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b9e0e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|loglevel|           logtime|\n",
      "+--------+------------------+\n",
      "|    INFO| 2015-8-8 20:49:22|\n",
      "|    WARN|2015-1-14 20:05:00|\n",
      "|    INFO|2017-6-14 00:08:35|\n",
      "|    INFO|2016-1-18 11:50:14|\n",
      "|   DEBUG| 2017-7-1 12:55:02|\n",
      "|    INFO|2014-2-26 12:34:21|\n",
      "|    INFO|2015-7-12 11:13:47|\n",
      "|    INFO|2017-4-15 01:20:18|\n",
      "|   DEBUG|2016-11-2 20:19:23|\n",
      "|    INFO|2012-8-20 10:09:44|\n",
      "|   DEBUG|2014-4-22 21:30:49|\n",
      "|    WARN|2013-12-6 17:54:15|\n",
      "|   DEBUG|2017-1-12 10:47:02|\n",
      "|   DEBUG|2016-6-25 11:06:42|\n",
      "|   ERROR|2015-6-28 19:25:05|\n",
      "|   DEBUG|2012-6-24 01:06:37|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bce811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loglevel: string (nullable = true)\n",
      " |-- logtime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1faea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loglevel: string (nullable = true)\n",
      " |-- logtime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "new_df=log_df.withColumn(\"logtime\",to_timestamp(\"logtime\"))\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1319fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|loglevel|       max(logtime)|\n",
      "+--------+-------------------+\n",
      "|    INFO|2017-06-14 00:08:35|\n",
      "|   ERROR|2015-06-28 19:25:05|\n",
      "|    WARN|2015-01-14 20:05:00|\n",
      "|   DEBUG|2017-07-01 12:55:02|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.groupBy(\"loglevel\").agg(max(\"logtime\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "790fa14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.createOrReplaceTempView(\"serverlogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9607976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|loglevel|            logtime|\n",
      "+--------+-------------------+\n",
      "|    INFO|2015-08-08 20:49:22|\n",
      "|    WARN|2015-01-14 20:05:00|\n",
      "|    INFO|2017-06-14 00:08:35|\n",
      "|    INFO|2016-01-18 11:50:14|\n",
      "|   DEBUG|2017-07-01 12:55:02|\n",
      "|    INFO|2014-02-26 12:34:21|\n",
      "|    INFO|2015-07-12 11:13:47|\n",
      "|    INFO|2017-04-15 01:20:18|\n",
      "|   DEBUG|2016-11-02 20:19:23|\n",
      "|    INFO|2012-08-20 10:09:44|\n",
      "|   DEBUG|2014-04-22 21:30:49|\n",
      "|    WARN|2013-12-06 17:54:15|\n",
      "|   DEBUG|2017-01-12 10:47:02|\n",
      "|   DEBUG|2016-06-25 11:06:42|\n",
      "|   ERROR|2015-06-28 19:25:05|\n",
      "|   DEBUG|2012-06-24 01:06:37|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from serverlogs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5d02ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|loglevel|   month|\n",
      "+--------+--------+\n",
      "|    INFO|  August|\n",
      "|    WARN| January|\n",
      "|    INFO|    June|\n",
      "|    INFO| January|\n",
      "|   DEBUG|    July|\n",
      "|    INFO|February|\n",
      "|    INFO|    July|\n",
      "|    INFO|   April|\n",
      "|   DEBUG|November|\n",
      "|    INFO|  August|\n",
      "|   DEBUG|   April|\n",
      "|    WARN|December|\n",
      "|   DEBUG| January|\n",
      "|   DEBUG|    June|\n",
      "|   ERROR|    June|\n",
      "|   DEBUG|    June|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select loglevel,date_format(logtime,'MMMM') as month  from serverlogs\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0027d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+\n",
      "|loglevel|   month|total_occurence|\n",
      "+--------+--------+---------------+\n",
      "|    INFO|    June|              1|\n",
      "|    WARN|December|              1|\n",
      "|   DEBUG|    July|              1|\n",
      "|    INFO|February|              1|\n",
      "|   ERROR|    June|              1|\n",
      "|    WARN| January|              1|\n",
      "|   DEBUG| January|              1|\n",
      "|    INFO|  August|              2|\n",
      "|   DEBUG|November|              1|\n",
      "|    INFO|   April|              1|\n",
      "|   DEBUG|    June|              2|\n",
      "|    INFO| January|              1|\n",
      "|   DEBUG|   April|              1|\n",
      "|    INFO|    July|              1|\n",
      "+--------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select loglevel,date_format(logtime,'MMMM') as month, count(*) as total_occurence\n",
    "from serverlogs\n",
    "group by loglevel,month\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d9c826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"loglevel string, logtime timestamp\"\n",
    "df = spark.read.format(\"csv\").\\\n",
    "option(\"inferSchema\",\"true\").\\\n",
    "schema(schema).\\\n",
    "load(\"/public/trendytech/datasets/logdata1m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "960bc9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|loglevel|            logtime|\n",
      "+--------+-------------------+\n",
      "|    INFO|2015-08-08 20:49:22|\n",
      "|    WARN|2015-01-14 20:05:00|\n",
      "|    INFO|2017-06-14 00:08:35|\n",
      "|    INFO|2016-01-18 11:50:14|\n",
      "|   DEBUG|2017-07-01 12:55:02|\n",
      "|    INFO|2014-02-26 12:34:21|\n",
      "|    INFO|2015-07-12 11:13:47|\n",
      "|    INFO|2017-04-15 01:20:18|\n",
      "|   DEBUG|2016-11-02 20:19:23|\n",
      "|    INFO|2012-08-20 10:09:44|\n",
      "|   DEBUG|2014-04-22 21:30:49|\n",
      "|    WARN|2013-12-06 17:54:15|\n",
      "|   DEBUG|2017-01-12 10:47:02|\n",
      "|   DEBUG|2016-06-25 11:06:42|\n",
      "|   ERROR|2015-06-28 19:25:05|\n",
      "|   DEBUG|2012-06-24 01:06:37|\n",
      "|    INFO|2014-12-09 09:53:54|\n",
      "|   DEBUG|2015-11-08 19:20:08|\n",
      "|    INFO|2017-07-21 18:34:18|\n",
      "|   DEBUG|2014-12-26 06:38:42|\n",
      "+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef6b3fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"serverlogs\")\n",
    "spark.sql(\"\"\"cache table serverlogs\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae757155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------------+\n",
      "|loglevel|    month|total_occurence|\n",
      "+--------+---------+---------------+\n",
      "|    WARN|     June|           8191|\n",
      "|    INFO|     June|          29143|\n",
      "|   ERROR| November|           3389|\n",
      "|   FATAL|  January|             94|\n",
      "|    WARN| December|           8328|\n",
      "|    WARN|    March|           8165|\n",
      "|   DEBUG|     July|          42085|\n",
      "|   ERROR|    April|           4107|\n",
      "|   ERROR|  January|           4054|\n",
      "|   FATAL|September|             81|\n",
      "|   FATAL|    April|             83|\n",
      "|    INFO|September|          29038|\n",
      "|   FATAL| November|          16797|\n",
      "|   FATAL|  October|             92|\n",
      "|    INFO| February|          28983|\n",
      "|    WARN|    April|           8277|\n",
      "|   DEBUG| December|          41749|\n",
      "|   FATAL| December|             94|\n",
      "|    WARN|      May|           8403|\n",
      "|   ERROR|     June|           4059|\n",
      "+--------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"\"\"select loglevel,date_format(logtime,'MMMM') as month, count(*) as total_occurence\n",
    "from serverlogs\n",
    "group by loglevel,month\"\"\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b61541bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "367e4d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70abb922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+\n",
      "|loglevel|   month|total_occurence|\n",
      "+--------+--------+---------------+\n",
      "|    WARN| January|           8217|\n",
      "|   DEBUG| January|          41961|\n",
      "|   FATAL| January|             94|\n",
      "|    INFO| January|          29119|\n",
      "|   ERROR| January|           4054|\n",
      "|   DEBUG|February|          41734|\n",
      "|    WARN|February|           8266|\n",
      "|    INFO|February|          28983|\n",
      "|   ERROR|February|           4013|\n",
      "|   FATAL|February|             72|\n",
      "|   FATAL|   March|             70|\n",
      "|   ERROR|   March|           4122|\n",
      "|   DEBUG|   March|          41652|\n",
      "|    WARN|   March|           8165|\n",
      "|    INFO|   March|          29095|\n",
      "|    INFO|   April|          29302|\n",
      "|   FATAL|   April|             83|\n",
      "|    WARN|   April|           8277|\n",
      "|   DEBUG|   April|          41869|\n",
      "|   ERROR|   April|           4107|\n",
      "+--------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"\"\"select loglevel,date_format(logtime,'MMMM') as month, \n",
    "cast(date_format(logtime,'M') as int) as month_num,\n",
    "count(*) as total_occurence\n",
    "from serverlogs\n",
    "group by loglevel,month,month_num order by month_num\"\"\").drop(\"month_num\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bebb0632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+\n",
      "|loglevel|   month|total_occurence|\n",
      "+--------+--------+---------------+\n",
      "|    INFO| January|          29119|\n",
      "|   DEBUG| January|          41961|\n",
      "|    WARN| January|           8217|\n",
      "|   FATAL| January|             94|\n",
      "|   ERROR| January|           4054|\n",
      "|   FATAL|February|             72|\n",
      "|   ERROR|February|           4013|\n",
      "|   DEBUG|February|          41734|\n",
      "|    WARN|February|           8266|\n",
      "|    INFO|February|          28983|\n",
      "|   ERROR|   March|           4122|\n",
      "|   DEBUG|   March|          41652|\n",
      "|   FATAL|   March|             70|\n",
      "|    WARN|   March|           8165|\n",
      "|    INFO|   March|          29095|\n",
      "|    WARN|   April|           8277|\n",
      "|   ERROR|   April|           4107|\n",
      "|    INFO|   April|          29302|\n",
      "|   FATAL|   April|             83|\n",
      "|   DEBUG|   April|          41869|\n",
      "+--------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"\"\"select loglevel,date_format(logtime,'MMMM') as month, \n",
    "date_format(logtime,'MM') as month_num,\n",
    "count(*) as total_occurence\n",
    "from serverlogs\n",
    "group by loglevel,month,month_num order by month_num\"\"\").drop(\"month_num\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49b59006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+\n",
      "|loglevel|   month|total_occurence|\n",
      "+--------+--------+---------------+\n",
      "|   DEBUG| January|          41961|\n",
      "|   FATAL| January|             94|\n",
      "|    INFO| January|          29119|\n",
      "|   ERROR| January|           4054|\n",
      "|    WARN| January|           8217|\n",
      "|    WARN|February|           8266|\n",
      "|   ERROR|February|           4013|\n",
      "|   DEBUG|February|          41734|\n",
      "|   FATAL|February|             72|\n",
      "|    INFO|February|          28983|\n",
      "|   ERROR|   March|           4122|\n",
      "|    WARN|   March|           8165|\n",
      "|    INFO|   March|          29095|\n",
      "|   DEBUG|   March|          41652|\n",
      "|   FATAL|   March|             70|\n",
      "|   ERROR|   April|           4107|\n",
      "|    WARN|   April|           8277|\n",
      "|   FATAL|   April|             83|\n",
      "|    INFO|   April|          29302|\n",
      "|   DEBUG|   April|          41869|\n",
      "+--------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"\"\"select loglevel,date_format(logtime,'MMMM') as month, \n",
    "first(date_format(logtime,'MM')) as month_num,\n",
    "count(*) as total_occurence\n",
    "from serverlogs\n",
    "group by loglevel,month order by month_num\"\"\").drop(\"month_num\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056571e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb2db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17b5e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b1a2998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+\n",
      "|loglevel|   month|total_occurence|\n",
      "+--------+--------+---------------+\n",
      "|   FATAL| January|             94|\n",
      "|   DEBUG| January|          41961|\n",
      "|    INFO| January|          29119|\n",
      "|   ERROR| January|           4054|\n",
      "|    WARN| January|           8217|\n",
      "|   ERROR|February|           4013|\n",
      "|    WARN|February|           8266|\n",
      "|   DEBUG|February|          41734|\n",
      "|   FATAL|February|             72|\n",
      "|    INFO|February|          28983|\n",
      "|   ERROR|   March|           4122|\n",
      "|    WARN|   March|           8165|\n",
      "|   DEBUG|   March|          41652|\n",
      "|   FATAL|   March|             70|\n",
      "|    INFO|   March|          29095|\n",
      "|    WARN|   April|           8277|\n",
      "|   ERROR|   April|           4107|\n",
      "|   FATAL|   April|             83|\n",
      "|    INFO|   April|          29302|\n",
      "|   DEBUG|   April|          41869|\n",
      "+--------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb2d323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|loglevel|   01|   02|   03|   04|   05|   06|   07|   08|   09|   10|   11|   12|\n",
      "+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|    INFO|29119|28983|29095|29302|28900|29143|29300|28993|29038|29018|23301|28874|\n",
      "|   ERROR| 4054| 4013| 4122| 4107| 4086| 4059| 3976| 3987| 4161| 4040| 3389| 4106|\n",
      "|    WARN| 8217| 8266| 8165| 8277| 8403| 8191| 8222| 8381| 8352| 8226| 6616| 8328|\n",
      "|   DEBUG|41961|41734|41652|41869|41785|41774|42085|42147|41433|41936|33366|41749|\n",
      "|   FATAL|   94|   72|   70|   83|   60|   78|   98|   80|   81|   92|16797|   94|\n",
      "+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.sql(\"\"\"select loglevel,date_format(logtime,'MM') as month\n",
    "from serverlogs\n",
    "\"\"\").groupBy(\"loglevel\").pivot('month').count()\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "916b7797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|loglevel|  Apr|  Aug|  Dec|  Feb|  Jan|  Jul|  Jun|  Mar|  May|  Nov|  Oct|  Sep|\n",
      "+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|    INFO|29302|28993|28874|28983|29119|29300|29143|29095|28900|23301|29018|29038|\n",
      "|   ERROR| 4107| 3987| 4106| 4013| 4054| 3976| 4059| 4122| 4086| 3389| 4040| 4161|\n",
      "|    WARN| 8277| 8381| 8328| 8266| 8217| 8222| 8191| 8165| 8403| 6616| 8226| 8352|\n",
      "|   FATAL|   83|   80|   94|   72|   94|   98|   78|   70|   60|16797|   92|   81|\n",
      "|   DEBUG|41869|42147|41749|41734|41961|42085|41774|41652|41785|33366|41936|41433|\n",
      "+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.sql(\"\"\"select loglevel,date_format(logtime,'MMM') as month\n",
    "from serverlogs\n",
    "\"\"\").groupBy(\"loglevel\").pivot('month').count()\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ccb7727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+----+\n",
      "|month|DEBUG|ERROR|FATAL| INFO|WARN|\n",
      "+-----+-----+-----+-----+-----+----+\n",
      "|  Oct|41936| 4040|   92|29018|8226|\n",
      "|  Sep|41433| 4161|   81|29038|8352|\n",
      "|  Dec|41749| 4106|   94|28874|8328|\n",
      "|  Aug|42147| 3987|   80|28993|8381|\n",
      "|  May|41785| 4086|   60|28900|8403|\n",
      "|  Jun|41774| 4059|   78|29143|8191|\n",
      "|  Feb|41734| 4013|   72|28983|8266|\n",
      "|  Nov|33366| 3389|16797|23301|6616|\n",
      "|  Mar|41652| 4122|   70|29095|8165|\n",
      "|  Jan|41961| 4054|   94|29119|8217|\n",
      "|  Apr|41869| 4107|   83|29302|8277|\n",
      "|  Jul|42085| 3976|   98|29300|8222|\n",
      "+-----+-----+-----+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.sql(\"\"\"select loglevel,date_format(logtime,'MMM') as month\n",
    "from serverlogs\n",
    "\"\"\").groupBy(\"month\").pivot('loglevel').count()\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb9108da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick way\n",
    "months_list = [\n",
    "    'January', 'February', 'March', 'April', 'May', 'June',\n",
    "    'July', 'August', 'September', 'October', 'November', 'December'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb966a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "|loglevel|January|February|March|April|  May| June| July|August|September|October|November|December|\n",
      "+--------+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "|    INFO|  29119|   28983|29095|29302|28900|29143|29300| 28993|    29038|  29018|   23301|   28874|\n",
      "|   ERROR|   4054|    4013| 4122| 4107| 4086| 4059| 3976|  3987|     4161|   4040|    3389|    4106|\n",
      "|    WARN|   8217|    8266| 8165| 8277| 8403| 8191| 8222|  8381|     8352|   8226|    6616|    8328|\n",
      "|   FATAL|     94|      72|   70|   83|   60|   78|   98|    80|       81|     92|   16797|      94|\n",
      "|   DEBUG|  41961|   41734|41652|41869|41785|41774|42085| 42147|    41433|  41936|   33366|   41749|\n",
      "+--------+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.sql(\"\"\"select loglevel,date_format(logtime,'MMMM') as month\n",
    "from serverlogs\n",
    "\"\"\").groupBy(\"loglevel\").pivot('month',months_list).count()\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b8781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
