{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d248b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>assignment-6</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4c60529908>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".master(\"yarn\")\\\n",
    ".appName(\"assignment-6\")\\\n",
    ".enableHiveSupport()\\\n",
    ".config(\"spark.sql.warehouse.dir\",\"/user/itv009490/warehouse\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6027a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list=[(\"Spring\",12.3),(\"Summer\",10.5),(\"Autumn\",8.2),(\"Winter\",15.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea6328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|season|windspeed|\n",
      "+------+---------+\n",
      "|Spring|     12.3|\n",
      "|Summer|     10.5|\n",
      "|Autumn|      8.2|\n",
      "|Winter|     15.1|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(my_list).toDF(\"season\",\"windspeed\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c98ad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: string (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc85045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5bf5e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"library_name\": \"Central Library\",\"location\": \"City Center\",\"books\": [{\"book_id\": \"B001\",\"book_name\": \"The Great Gatsby\",\"author\": \"F. Scott Fitzgerald\",\"copies_available\": 5},{\"book_id\": \"B002\",\"book_name\": \"To Kill a Mockingbird\",\"author\": \"Harper Lee\",\"copies_available\": 3}],\"members\": [{\"member_id\": \"M001\",\"member_name\": \"John Smith\",\"age\": 28,\"books_borrowed\": [\"B001\"]},{\"member_id\": \"M002\",\"member_name\": \"Emma Johnson\",\"age\": 35,\"books_borrowed\": []}]},\n",
      "{\"library_name\": \"Community Library\",\"location\": \"Suburb\",\"books\": [{\"book_id\": \"B003\",\"book_name\": \"1984\",\"author\": \"George Orwell\",\"copies_available\": 2},{\"book_id\": \"B004\",\"book_name\": \"Pride and Prejudice\",\"author\": \"Jane Austen\",\"copies_available\": 4}],\"members\": [{\"member_id\": \"M003\",\"member_name\": \"Michael Brown\",\"age\": 42,\"books_borrowed\": [\"B003\",\"B004\"]},{\"member_id\": \"M004\",\"member_name\": \"Sophia Davis\",\"age\": 31,\"books_borrowed\": [\"B004\"]}]}\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head /public/trendytech/datasets/library_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7c5bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "struct_schema = StructType([\n",
    "    StructField(\"library_name\",StringType()),\n",
    "    StructField(\"location\",StringType()),\n",
    "    StructField(\"books\",ArrayType(StructType([\n",
    "        StructField(\"book_id\",StringType()),\n",
    "        StructField(\"book_name\",StringType()),\n",
    "        StructField(\"author\",StringType()),\n",
    "        StructField(\"copies_available\",IntegerType())\n",
    "    ])\n",
    "                                 )),\n",
    "    StructField(\"members\",ArrayType(StructType([\n",
    "        StructField(\"member_id\",StringType()),\n",
    "        StructField(\"member_name\",StringType()),\n",
    "        StructField(\"age\",IntegerType()),\n",
    "        StructField(\"books_borrowed\",ArrayType(StringType()))\n",
    "    ])\n",
    "                                   ))\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63164551",
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_df = spark.read.format(\"json\").schema(struct_schema).load(\"/public/trendytech/datasets/library_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e46be14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n",
      "|library_name     |location   |books                                                                                           |members                                                                    |\n",
      "+-----------------+-----------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n",
      "|Central Library  |City Center|[{B001, The Great Gatsby, F. Scott Fitzgerald, 5}, {B002, To Kill a Mockingbird, Harper Lee, 3}]|[{M001, John Smith, 28, [B001]}, {M002, Emma Johnson, 35, []}]             |\n",
      "|Community Library|Suburb     |[{B003, 1984, George Orwell, 2}, {B004, Pride and Prejudice, Jane Austen, 4}]                   |[{M003, Michael Brown, 42, [B003, B004]}, {M004, Sophia Davis, 31, [B004]}]|\n",
      "+-----------------+-----------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lib_df.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d43e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd78cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1cc0f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_number,train_name,seats_available,passenger_name,age,ticket_number,seat_number\n",
      "123,Express,100,John,25,T123,A1\n",
      "123,Express,100,Emma,30,T124,B2\n",
      "456,Superfast,150,Michael,35,T125,C3\n",
      "456,Superfast,150,Sophia,40,T126,D4\n",
      "789,Local,50,William,28,T127,E5\n",
      "789,Local,50,Sophia,32,T128,F6\n",
      "789,Local,50,Oliver,45,T129,G7\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head /public/trendytech/datasets/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fce327b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|train_number|train_name|seats_available|passenger_name|age|ticket_number|seat_number|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|         123|   Express|            100|          John| 25|         T123|         A1|\n",
      "|         123|   Express|            100|          Emma| 30|         T124|         B2|\n",
      "|         456| Superfast|            150|       Michael| 35|         T125|         C3|\n",
      "|         456| Superfast|            150|        Sophia| 40|         T126|         D4|\n",
      "|         789|     Local|             50|       William| 28|         T127|         E5|\n",
      "|         789|     Local|             50|        Sophia| 32|         T128|         F6|\n",
      "|         789|     Local|             50|        Oliver| 45|         T129|         G7|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.format(\"csv\").\\\n",
    "option(\"inferSchema\",\"true\").\\\n",
    "option(\"header\",\"true\").\\\n",
    "load(\"/public/trendytech/datasets/train.csv\")\n",
    "\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9f7de66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train_number: integer (nullable = true)\n",
      " |-- train_name: string (nullable = true)\n",
      " |-- seats_available: integer (nullable = true)\n",
      " |-- passenger_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- ticket_number: string (nullable = true)\n",
      " |-- seat_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9efbbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-------------+-----------+\n",
      "|train_number|train_name|seats_available|ticket_number|seat_number|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "|         123|   Express|            100|         T123|         A1|\n",
      "|         123|   Express|            100|         T124|         B2|\n",
      "|         456| Superfast|            150|         T125|         C3|\n",
      "|         456| Superfast|            150|         T126|         D4|\n",
      "|         789|     Local|             50|         T127|         E5|\n",
      "|         789|     Local|             50|         T128|         F6|\n",
      "|         789|     Local|             50|         T129|         G7|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = train_df.drop(\"passenger_name\",\"age\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bea0483c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "655f2ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df1.dropDuplicates([\"train_number\",\"ticket_number\"])\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc84a8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-------------+-----------+\n",
      "|train_number|train_name|seats_available|ticket_number|seat_number|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "|         789|     Local|             50|         T128|         F6|\n",
      "|         123|   Express|            100|         T124|         B2|\n",
      "|         123|   Express|            100|         T123|         A1|\n",
      "|         456| Superfast|            150|         T126|         D4|\n",
      "|         456| Superfast|            150|         T125|         C3|\n",
      "|         789|     Local|             50|         T127|         E5|\n",
      "|         789|     Local|             50|         T129|         G7|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18fd1cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select(\"train_name\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572bc14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b6e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "834a7b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"store_id\": 1, \"product\": \"Apple\", \"quantity\": 10, \"revenue\": 100.0}\n",
      "{\"store_id\": 2, \"product\": \"Banana\", \"quantity\": 15, \"revenue\": 75.0}\n",
      "{\"store_id\": 3, \"product\": \"Orange\", \"quantity\": 12, \"revenue\": 90.0}\n",
      "{\"store_id\": 4, \"product\": \"Mango\", \"quantity\": 8, \"revenue\": 120.0}\n",
      "{\"store_id\": 5, \"product\": \"Grape\", \"quantity\": 20, \"revenue\": 150.0}\n",
      "{\"store_id\": 6, \"product\": \"Watermelon\", \"quantity\": 5, \"revenue\": 50.0}\n",
      "{\"store_id\": 7, \"product\": \"Strawberry\", \"quantity\": 18, \"revenue\": 108.0}\n",
      "{\"store_id\": 8, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0}\n",
      "{\"store_id\": 9, \"product\": \"Cherry\", \"quantity\": 7, \"revenue\": 105.0}\n",
      "{\"store_id\": 10, \"product\": \"Pear\", \"quantity\": 9, \"revenue\": 81.0}\n",
      "{\"store_id\": 11, \"product\": \"Blueberry\", \"quantity\": 11, \"revenue\": 88.0}\n",
      "{\"store_id\": 12, \"product\": \"Kiwi\", \"quantity\": 16, \"revenue\": 128.0}\n",
      "{\"store_id\": 13, \"product\": \"Peach\", \"quantity\": 13, \"revenue\": 91.0}\n",
      "{\"store_id\": 14, \"product\": \"Plum\", \"quantity\": 6, \"revenue\": 54.0}\n",
      "{\"store_id\": 15, \"p"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head /public/trendytech/datasets/sales_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3d82386",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = 'store_id integer, product string, quantity integer, revenue double'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10338c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").\\\n",
    "schema(schema1).\\\n",
    "option(\"mode\",\"permissive\").\\\n",
    "load(\"/public/trendytech/datasets/sales_data.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76262185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a594ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropmalformed=spark.read.option(\"mode\",\"dropmalformed\").schema(schema1).json(\"/public/trendytech/datasets/sales_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69fd3d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dropmalformed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d057172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------+\n",
      "|store_id|   product|quantity|revenue|\n",
      "+--------+----------+--------+-------+\n",
      "|       1|     Apple|      10|  100.0|\n",
      "|       2|    Banana|      15|   75.0|\n",
      "|       3|    Orange|      12|   90.0|\n",
      "|       4|     Mango|       8|  120.0|\n",
      "|       5|     Grape|      20|  150.0|\n",
      "|       6|Watermelon|       5|   50.0|\n",
      "|       7|Strawberry|      18|  108.0|\n",
      "|       8| Pineapple|      14|  140.0|\n",
      "|       9|    Cherry|       7|  105.0|\n",
      "|      10|      Pear|       9|   81.0|\n",
      "|      11| Blueberry|      11|   88.0|\n",
      "|      12|      Kiwi|      16|  128.0|\n",
      "|      13|     Peach|      13|   91.0|\n",
      "|      14|      Plum|       6|   54.0|\n",
      "|      15|     Lemon|      10|   70.0|\n",
      "|      16| Raspberry|      17|  136.0|\n",
      "|      17|   Coconut|       4|   80.0|\n",
      "|      18|   Avocado|      11|   99.0|\n",
      "|      19|Blackberry|       8|   64.0|\n",
      "+--------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dropmalformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0071814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failfast=spark.read.option(\"mode\",\"failfast\").schema(schema1).json(\"/public/trendytech/datasets/sales_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aefb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa300f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a6f5be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_id,admission_date,discharge_date,diagnosis,doctor_id,total_cost\n",
      "1,01-01-2022,2022-01-10,Pneumonia,101,5000.00\n",
      "2,02-05-2022,2022-02-09,Appendicitis,102,7000.00\n",
      "3,03-12-2022,2022-03-18,Fractured Arm,103,3500.00\n",
      "4,04-02-2022,2022-04-08,Heart Attack,104,15000.00\n",
      "5,05-05-2022,2022-05-07,Influenza,105,2500.00\n",
      "6,06-10-2022,2022-06-15,Appendicitis,106,8000.00\n",
      "7,07-20-2022,2022-07-25,Pneumonia,107,5500.00\n",
      "8,08-25-2022,2022-09-01,Heart Attack,108,20000.00\n",
      "9,09-15-2022,2022-09-22,Fractured Leg,109,6000.00\n",
      "10,10-05-2022,2022-10-10,Appendicitis,110,7500.00\n",
      "11,11-02-2022,2022-11-05,Influenza,111,2800.00\n",
      "12,12-10-2022,2022-12-18,Pneumonia,112,6000.00\n",
      "13,01-02-2023,2023-01-09,Heart Attack,113,18000.00\n",
      "14,02-14-2023,2023-02-18,Appendicitis,114,7200.00\n",
      "15,03-20-2023,2023-03-28,Fractured Arm,115,3800.00\n",
      "16,04-05-2023,2023-04-11,Influenza,116,2700.00\n",
      "17,05-08-2023,2023-05-11,Heart Attack,117,16000.00\n",
      "18,06-15-2023,2023-06-20,Pneumonia,118,4800.00\n",
      "19,07-22-2023,2023-07-27,Fractured Leg,119,6500.00\n",
      "20,0"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head /public/trendytech/datasets/hospital.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d959245",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1=\"patient_id integer, admission_date date, discharge_date date, diagnosis string,doctor_id integer, total_cost  float\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "793d1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"dateFormat\",\"mm-dd-yyyy\").schema(schema1).load(\"/public/trendytech/datasets/hospital.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "860eedef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|      101|    5000.0|\n",
      "|         2|    2022-01-05|    2022-02-09| Appendicitis|      102|    7000.0|\n",
      "|         3|    2022-01-12|    2022-03-18|Fractured Arm|      103|    3500.0|\n",
      "|         4|    2022-01-02|    2022-04-08| Heart Attack|      104|   15000.0|\n",
      "|         5|    2022-01-05|    2022-05-07|    Influenza|      105|    2500.0|\n",
      "|         6|    2022-01-10|    2022-06-15| Appendicitis|      106|    8000.0|\n",
      "|         7|    2022-01-20|    2022-07-25|    Pneumonia|      107|    5500.0|\n",
      "|         8|    2022-01-25|    2022-09-01| Heart Attack|      108|   20000.0|\n",
      "|         9|    2022-01-15|    2022-09-22|Fractured Leg|      109|    6000.0|\n",
      "|        10|    2022-01-05|    2022-10-10| Appendicitis|      110|    7500.0|\n",
      "|        11|    2022-01-02|    2022-11-05|    Influenza|      111|    2800.0|\n",
      "|        12|    2022-01-10|    2022-12-18|    Pneumonia|      112|    6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      113|   18000.0|\n",
      "|        14|    2023-01-14|    2023-02-18| Appendicitis|      114|    7200.0|\n",
      "|        15|    2023-01-20|    2023-03-28|Fractured Arm|      115|    3800.0|\n",
      "|        16|    2023-01-05|    2023-04-11|    Influenza|      116|    2700.0|\n",
      "|        17|    2023-01-08|    2023-05-11| Heart Attack|      117|   16000.0|\n",
      "|        18|    2023-01-15|    2023-06-20|    Pneumonia|      118|    4800.0|\n",
      "|        19|    2023-01-22|    2023-07-27|Fractured Leg|      119|    6500.0|\n",
      "|        20|    2023-01-10|    2023-08-16| Appendicitis|      120|    7800.0|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "493a1fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|total_cost|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|    5000.0|\n",
      "|         2|    2022-01-05|    2022-02-09| Appendicitis|    7000.0|\n",
      "|         3|    2022-01-12|    2022-03-18|Fractured Arm|    3500.0|\n",
      "|         4|    2022-01-02|    2022-04-08| Heart Attack|   15000.0|\n",
      "|         5|    2022-01-05|    2022-05-07|    Influenza|    2500.0|\n",
      "|         6|    2022-01-10|    2022-06-15| Appendicitis|    8000.0|\n",
      "|         7|    2022-01-20|    2022-07-25|    Pneumonia|    5500.0|\n",
      "|         8|    2022-01-25|    2022-09-01| Heart Attack|   20000.0|\n",
      "|         9|    2022-01-15|    2022-09-22|Fractured Leg|    6000.0|\n",
      "|        10|    2022-01-05|    2022-10-10| Appendicitis|    7500.0|\n",
      "|        11|    2022-01-02|    2022-11-05|    Influenza|    2800.0|\n",
      "|        12|    2022-01-10|    2022-12-18|    Pneumonia|    6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|   18000.0|\n",
      "|        14|    2023-01-14|    2023-02-18| Appendicitis|    7200.0|\n",
      "|        15|    2023-01-20|    2023-03-28|Fractured Arm|    3800.0|\n",
      "|        16|    2023-01-05|    2023-04-11|    Influenza|    2700.0|\n",
      "|        17|    2023-01-08|    2023-05-11| Heart Attack|   16000.0|\n",
      "|        18|    2023-01-15|    2023-06-20|    Pneumonia|    4800.0|\n",
      "|        19|    2023-01-22|    2023-07-27|Fractured Leg|    6500.0|\n",
      "|        20|    2023-01-10|    2023-08-16| Appendicitis|    7800.0|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.drop(\"doctor_id\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a4a0d5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|\n",
      "|         2|    2022-01-05|    2022-02-09| Appendicitis|       7000.0|\n",
      "|         3|    2022-01-12|    2022-03-18|Fractured Arm|       3500.0|\n",
      "|         4|    2022-01-02|    2022-04-08| Heart Attack|      15000.0|\n",
      "|         5|    2022-01-05|    2022-05-07|    Influenza|       2500.0|\n",
      "|         6|    2022-01-10|    2022-06-15| Appendicitis|       8000.0|\n",
      "|         7|    2022-01-20|    2022-07-25|    Pneumonia|       5500.0|\n",
      "|         8|    2022-01-25|    2022-09-01| Heart Attack|      20000.0|\n",
      "|         9|    2022-01-15|    2022-09-22|Fractured Leg|       6000.0|\n",
      "|        10|    2022-01-05|    2022-10-10| Appendicitis|       7500.0|\n",
      "|        11|    2022-01-02|    2022-11-05|    Influenza|       2800.0|\n",
      "|        12|    2022-01-10|    2022-12-18|    Pneumonia|       6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      18000.0|\n",
      "|        14|    2023-01-14|    2023-02-18| Appendicitis|       7200.0|\n",
      "|        15|    2023-01-20|    2023-03-28|Fractured Arm|       3800.0|\n",
      "|        16|    2023-01-05|    2023-04-11|    Influenza|       2700.0|\n",
      "|        17|    2023-01-08|    2023-05-11| Heart Attack|      16000.0|\n",
      "|        18|    2023-01-15|    2023-06-20|    Pneumonia|       4800.0|\n",
      "|        19|    2023-01-22|    2023-07-27|Fractured Leg|       6500.0|\n",
      "|        20|    2023-01-10|    2023-08-16| Appendicitis|       7800.0|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumnRenamed(\"total_cost\",\"hospital_bill\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5bf1c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|          9 days|\n",
      "|         2|    2022-01-05|    2022-02-09| Appendicitis|       7000.0| 1 months 4 days|\n",
      "|         3|    2022-01-12|    2022-03-18|Fractured Arm|       3500.0| 2 months 6 days|\n",
      "|         4|    2022-01-02|    2022-04-08| Heart Attack|      15000.0| 3 months 6 days|\n",
      "|         5|    2022-01-05|    2022-05-07|    Influenza|       2500.0| 4 months 2 days|\n",
      "|         6|    2022-01-10|    2022-06-15| Appendicitis|       8000.0| 5 months 5 days|\n",
      "|         7|    2022-01-20|    2022-07-25|    Pneumonia|       5500.0| 6 months 5 days|\n",
      "|         8|    2022-01-25|    2022-09-01| Heart Attack|      20000.0| 7 months 7 days|\n",
      "|         9|    2022-01-15|    2022-09-22|Fractured Leg|       6000.0| 8 months 7 days|\n",
      "|        10|    2022-01-05|    2022-10-10| Appendicitis|       7500.0| 9 months 5 days|\n",
      "|        11|    2022-01-02|    2022-11-05|    Influenza|       2800.0|10 months 3 days|\n",
      "|        12|    2022-01-10|    2022-12-18|    Pneumonia|       6000.0|11 months 8 days|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      18000.0|          7 days|\n",
      "|        14|    2023-01-14|    2023-02-18| Appendicitis|       7200.0| 1 months 4 days|\n",
      "|        15|    2023-01-20|    2023-03-28|Fractured Arm|       3800.0| 2 months 8 days|\n",
      "|        16|    2023-01-05|    2023-04-11|    Influenza|       2700.0| 3 months 6 days|\n",
      "|        17|    2023-01-08|    2023-05-11| Heart Attack|      16000.0| 4 months 3 days|\n",
      "|        18|    2023-01-15|    2023-06-20|    Pneumonia|       4800.0| 5 months 5 days|\n",
      "|        19|    2023-01-22|    2023-07-27|Fractured Leg|       6500.0| 6 months 5 days|\n",
      "|        20|    2023-01-10|    2023-08-16| Appendicitis|       7800.0| 7 months 6 days|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df3 = df2.withColumn(\"duration_of_stay\",expr(\"discharge_date -admission_date \"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b34c4648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|adjusted_total_cost|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|          9 days|             5000.0|\n",
      "|         2|    2022-01-05|    2022-02-09| Appendicitis|       7000.0| 1 months 4 days|             8400.0|\n",
      "|         3|    2022-01-12|    2022-03-18|Fractured Arm|       3500.0| 2 months 6 days|             3500.0|\n",
      "|         4|    2022-01-02|    2022-04-08| Heart Attack|      15000.0| 3 months 6 days|            22500.0|\n",
      "|         5|    2022-01-05|    2022-05-07|    Influenza|       2500.0| 4 months 2 days|             2500.0|\n",
      "|         6|    2022-01-10|    2022-06-15| Appendicitis|       8000.0| 5 months 5 days|             9600.0|\n",
      "|         7|    2022-01-20|    2022-07-25|    Pneumonia|       5500.0| 6 months 5 days|             5500.0|\n",
      "|         8|    2022-01-25|    2022-09-01| Heart Attack|      20000.0| 7 months 7 days|            30000.0|\n",
      "|         9|    2022-01-15|    2022-09-22|Fractured Leg|       6000.0| 8 months 7 days|             6000.0|\n",
      "|        10|    2022-01-05|    2022-10-10| Appendicitis|       7500.0| 9 months 5 days|             9000.0|\n",
      "|        11|    2022-01-02|    2022-11-05|    Influenza|       2800.0|10 months 3 days|             2800.0|\n",
      "|        12|    2022-01-10|    2022-12-18|    Pneumonia|       6000.0|11 months 8 days|             6000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      18000.0|          7 days|            27000.0|\n",
      "|        14|    2023-01-14|    2023-02-18| Appendicitis|       7200.0| 1 months 4 days|             8640.0|\n",
      "|        15|    2023-01-20|    2023-03-28|Fractured Arm|       3800.0| 2 months 8 days|             3800.0|\n",
      "|        16|    2023-01-05|    2023-04-11|    Influenza|       2700.0| 3 months 6 days|             2700.0|\n",
      "|        17|    2023-01-08|    2023-05-11| Heart Attack|      16000.0| 4 months 3 days|            24000.0|\n",
      "|        18|    2023-01-15|    2023-06-20|    Pneumonia|       4800.0| 5 months 5 days|             4800.0|\n",
      "|        19|    2023-01-22|    2023-07-27|Fractured Leg|       6500.0| 6 months 5 days|             6500.0|\n",
      "|        20|    2023-01-10|    2023-08-16| Appendicitis|       7800.0| 7 months 6 days|             9360.0|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.withColumn(\"adjusted_total_cost\",expr(\"\"\"case when diagnosis like '%Heart Attack%' then hospital_bill*1.5\n",
    "when diagnosis like '%Appendicitis%' then hospital_bill*1.2\n",
    "else hospital_bill end\n",
    "\"\"\"))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89cc1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+-------------------+\n",
      "|patient_id|    diagnosis|hospital_bill|adjusted_total_cost|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "|         1|    Pneumonia|       5000.0|             5000.0|\n",
      "|         2| Appendicitis|       7000.0|             8400.0|\n",
      "|         3|Fractured Arm|       3500.0|             3500.0|\n",
      "|         4| Heart Attack|      15000.0|            22500.0|\n",
      "|         5|    Influenza|       2500.0|             2500.0|\n",
      "|         6| Appendicitis|       8000.0|             9600.0|\n",
      "|         7|    Pneumonia|       5500.0|             5500.0|\n",
      "|         8| Heart Attack|      20000.0|            30000.0|\n",
      "|         9|Fractured Leg|       6000.0|             6000.0|\n",
      "|        10| Appendicitis|       7500.0|             9000.0|\n",
      "|        11|    Influenza|       2800.0|             2800.0|\n",
      "|        12|    Pneumonia|       6000.0|             6000.0|\n",
      "|        13| Heart Attack|      18000.0|            27000.0|\n",
      "|        14| Appendicitis|       7200.0|             8640.0|\n",
      "|        15|Fractured Arm|       3800.0|             3800.0|\n",
      "|        16|    Influenza|       2700.0|             2700.0|\n",
      "|        17| Heart Attack|      16000.0|            24000.0|\n",
      "|        18|    Pneumonia|       4800.0|             4800.0|\n",
      "|        19|Fractured Leg|       6500.0|             6500.0|\n",
      "|        20| Appendicitis|       7800.0|             9360.0|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5=df4.select(\"patient_id\",\"diagnosis\",\"hospital_bill\",\"adjusted_total_cost\")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528d1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
